{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":107515,"databundleVersionId":13015829,"sourceType":"competition"},{"sourceId":472251,"sourceType":"modelInstanceVersion","modelInstanceId":380611,"modelId":400325}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":10001.207176,"end_time":"2025-07-12T23:17:26.229115","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-12T20:30:45.021939","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport random\nimport collections\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torchvision\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":21.591789,"end_time":"2025-07-12T20:31:12.992046","exception":false,"start_time":"2025-07-12T20:30:51.400257","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nfix_all_seeds(2025)","metadata":{"papermill":{"duration":0.019977,"end_time":"2025-07-12T20:31:13.015514","exception":false,"start_time":"2025-07-12T20:31:12.995537","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_CSV = \"/kaggle/input/kaust-vs-kku-tournament-round-3/cells_segmentation/train.csv\"\nTRAIN_PATH = \"/kaggle/input/kaust-vs-kku-tournament-round-3/cells_segmentation/train\"\nTEST_PATH = \"/kaggle/input/kaust-vs-kku-tournament-round-3/cells_segmentation/test\"\nUNLABELED_PATH = \"/kaggle/input/kaust-vs-kku-tournament-round-3/cells_segmentation/unlabeled_additional_data\"\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nWIDTH = 704\nHEIGHT = 520\nPCT_IMAGES_VALIDATION = 0.1\n\nBATCH_SIZE = 2 \nGRADIENT_ACCUMULATION_STEPS = 2  \nNUM_EPOCHS = 30\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 1e-4\nBOX_DETECTIONS_PER_IMG = 160  \nWARMUP_EPOCHS = 3\n\nUSE_AMP = True\n\nBOXES_CONF = 0.2\nMASK_THRESHOLD = 0.5","metadata":{"papermill":{"duration":0.098854,"end_time":"2025-07-12T20:31:13.117722","exception":false,"start_time":"2025-07-12T20:31:13.018868","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_transform(train=True, height=HEIGHT, width=WIDTH):\n    if train:\n        return A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.Transpose(p=0.5),\n            \n            A.OneOf([\n                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n                A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n            ], p=0.3), \n            \n            A.Resize(height, width, always_apply=True),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], always_apply=True),\n            ToTensorV2(),\n        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n    else:\n        return A.Compose([\n            A.Resize(height, width, always_apply=True),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], always_apply=True),\n            ToTensorV2(),\n        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))","metadata":{"papermill":{"duration":0.012323,"end_time":"2025-07-12T20:31:13.133456","exception":false,"start_time":"2025-07-12T20:31:13.121133","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rle_decode(mask_rle, shape, color=1):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n\ndef rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b > prev + 1): \n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask","metadata":{"papermill":{"duration":0.010886,"end_time":"2025-07-12T20:31:13.147337","exception":false,"start_time":"2025-07-12T20:31:13.136451","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CellDataset(Dataset):\n    def __init__(self, image_dir, df, transforms=None):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = df\n        self.height = HEIGHT\n        self.width = WIDTH\n        \n        self.image_info = collections.defaultdict(dict)\n        temp_df = self.df.groupby('id')['annotation'].agg(lambda x: list(x)).reset_index()\n        for index, row in temp_df.iterrows():\n            self.image_info[index] = {\n                'image_id': row['id'],\n                'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n                'annotations': row[\"annotation\"]\n            }\n    \n    def get_box(self, a_mask):\n        pos = np.where(a_mask)\n        if len(pos[0]) == 0:\n            return [0, 0, 1, 1]\n        xmin = np.min(pos[1])\n        xmax = np.max(pos[1])\n        ymin = np.min(pos[0])\n        ymax = np.max(pos[0])\n        return [xmin, ymin, xmax, ymax]\n\n    def __getitem__(self, idx):\n        info = self.image_info[idx]\n        img = Image.open(info['image_path']).convert(\"RGB\")\n        img_np = np.array(img)\n\n        masks = []\n        boxes = []\n        for rle in info['annotations']:\n            mask = rle_decode(rle, (self.height, self.width)).astype('uint8')\n            if mask.sum() == 0:\n                continue\n            masks.append(mask)\n            boxes.append(self.get_box(mask))\n\n        if len(masks) == 0:\n            masks = [np.zeros((self.height, self.width), dtype=np.uint8)]\n            boxes = [[0, 0, 1, 1]]\n\n        labels = [1] * len(masks)\n\n        if self.transforms:\n            try:\n                augmented = self.transforms(\n                    image=img_np,\n                    masks=masks,\n                    bboxes=boxes,\n                    labels=labels\n                )\n                img = augmented['image']\n                masks = [torch.as_tensor(m, dtype=torch.uint8) for m in augmented['masks']]\n                boxes = torch.as_tensor(augmented['bboxes'], dtype=torch.float32)\n                labels = torch.as_tensor(augmented['labels'], dtype=torch.int64)\n            except:\n                img = torchvision.transforms.ToTensor()(img_np)\n                masks = [torch.as_tensor(m, dtype=torch.uint8) for m in masks]\n                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n                labels = torch.as_tensor(labels, dtype=torch.int64)\n        else:\n            img = torchvision.transforms.ToTensor()(img_np)\n            masks = [torch.as_tensor(m, dtype=torch.uint8) for m in masks]\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        if len(masks) == 0:\n            masks = [torch.zeros((self.height, self.width), dtype=torch.uint8)]\n            boxes = torch.tensor([[0, 0, 1, 1]], dtype=torch.float32)\n            labels = torch.tensor([1], dtype=torch.int64)\n\n        masks = torch.stack(masks)\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((labels.shape[0],), dtype=torch.int64)\n\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'masks': masks,\n            'image_id': image_id,\n            'area': area,\n            'iscrowd': iscrowd\n        }\n\n        return img, target\n\n    def __len__(self):\n        return len(self.image_info)","metadata":{"papermill":{"duration":0.017067,"end_time":"2025-07-12T20:31:13.167300","exception":false,"start_time":"2025-07-12T20:31:13.150233","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndf_base = pd.read_csv(TRAIN_CSV)\nunique_ids = df_base['id'].unique()\ntrain_ids, val_ids = train_test_split(\n    unique_ids,\n    test_size=PCT_IMAGES_VALIDATION,\n    random_state=42,\n    shuffle=True\n)\n\ndf_train = df_base[df_base['id'].isin(train_ids)]\ndf_val = df_base[df_base['id'].isin(val_ids)]\n\nprint(f\"Train images: {len(train_ids)}, Val images: {len(val_ids)}\")\nprint(f\"Train annotations: {len(df_train)}, Val annotations: {len(df_val)}\")\n\nds_train = CellDataset(TRAIN_PATH, df_base, transforms=get_transform(train=True))\ndl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, \n                      num_workers=2, collate_fn=collate_fn, pin_memory=True)\n\nds_val = CellDataset(TRAIN_PATH, df_val, transforms=get_transform(train=False))\ndl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, \n                    num_workers=2, collate_fn=collate_fn, pin_memory=True)","metadata":{"papermill":{"duration":0.425607,"end_time":"2025-07-12T20:31:13.595781","exception":false,"start_time":"2025-07-12T20:31:13.170174","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model():\n    NUM_CLASSES = 2\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n        pretrained=True,\n        box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n        box_score_thresh=0.05,\n        box_nms_thresh=0.5,  \n        rpn_score_thresh=0.05,\n        rpn_nms_thresh=0.7,\n        rpn_pre_nms_top_n_train=1700,  \n        rpn_pre_nms_top_n_test=800,  \n        rpn_post_nms_top_n_train=1700, \n        rpn_post_nms_top_n_test=800    \n    )\n    \n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, NUM_CLASSES)\n    \n    # Enable gradient checkpointing to save memory\n    model.backbone.body.requires_grad_(True)\n    \n    return model","metadata":{"papermill":{"duration":2.159653,"end_time":"2025-07-12T20:31:15.758705","exception":false,"start_time":"2025-07-12T20:31:13.599052","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, warmup_scheduler=None):\n    model.train()\n    loss_meter = AverageMeter()\n    mask_loss_meter = AverageMeter()\n    \n    scaler = torch.cuda.amp.GradScaler() if USE_AMP else None\n    optimizer.zero_grad()\n    \n    for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc=f\"Epoch {epoch}\")):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        if USE_AMP:\n            with torch.cuda.amp.autocast():\n                loss_dict = model(images, targets)\n                loss = sum(loss for loss in loss_dict.values())\n                loss = loss / GRADIENT_ACCUMULATION_STEPS\n        else:\n            loss_dict = model(images, targets)\n            loss = sum(loss for loss in loss_dict.values())\n            loss = loss / GRADIENT_ACCUMULATION_STEPS\n        \n        if USE_AMP:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n        \n        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            if USE_AMP:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n                optimizer.step()\n            \n            optimizer.zero_grad()\n            \n            if warmup_scheduler is not None and epoch <= WARMUP_EPOCHS:\n                warmup_scheduler.step()\n        \n        loss_meter.update(loss.item() * GRADIENT_ACCUMULATION_STEPS, len(images))\n        if 'loss_mask' in loss_dict:\n            mask_loss_meter.update(loss_dict['loss_mask'].item(), len(images))\n        \n        # Clear cache every 10 batches\n        if batch_idx % 10 == 0:\n            torch.cuda.empty_cache()\n    \n    return loss_meter.avg, mask_loss_meter.avg\n            \n\ndef validate_one_epoch(model, data_loader, device):\n    model.train()  # Keep in train mode to get losses\n    loss_meter = AverageMeter()\n    mask_loss_meter = AverageMeter()\n    \n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc=\"Validation\")):\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            if USE_AMP:\n                with torch.cuda.amp.autocast():\n                    loss_dict = model(images, targets)\n                    loss = sum(loss for loss in loss_dict.values())\n            else:\n                loss_dict = model(images, targets)\n                loss = sum(loss for loss in loss_dict.values())\n            \n            loss_meter.update(loss.item(), len(images))\n            if 'loss_mask' in loss_dict:\n                mask_loss_meter.update(loss_dict['loss_mask'].item(), len(images))\n            \n            # Clear cache every 5 batches during validation\n            if batch_idx % 5 == 0:\n                torch.cuda.empty_cache()\n    \n    return loss_meter.avg, mask_loss_meter.avg","metadata":{"papermill":{"duration":0.018066,"end_time":"2025-07-12T20:31:15.797361","exception":false,"start_time":"2025-07-12T20:31:15.779295","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\nimport gc\ngc.collect()\n\nprint(f\"GPU Memory before training: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")","metadata":{"papermill":{"duration":0.203328,"end_time":"2025-07-12T20:31:16.005170","exception":false,"start_time":"2025-07-12T20:31:15.801842","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"old_model_weights = torch.load('/kaggle/input/model_epoch_20/pytorch/default/1/model_epoch_20.pth')\nmodel = get_model()\nmodel.load_state_dict(old_model_weights['model_state_dict'])\nmodel = model.to(DEVICE)\n\nprint(f\"Loaded best model from epoch {old_model_weights['epoch']} with val_loss: {old_model_weights['val_loss']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.AdamW(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\ndef warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) / warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n\ndef cosine_lr_scheduler(optimizer, total_epochs, warmup_epochs):\n    warmup_iters = warmup_epochs * len(dl_train)\n    warmup_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, 0.1)\n    main_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=total_epochs - warmup_epochs, eta_min=LEARNING_RATE * 0.1\n    )\n    return warmup_scheduler, main_scheduler\n\nwarmup_scheduler, main_scheduler = cosine_lr_scheduler(optimizer, NUM_EPOCHS, WARMUP_EPOCHS)","metadata":{"papermill":{"duration":0.01253,"end_time":"2025-07-12T20:31:15.775378","exception":false,"start_time":"2025-07-12T20:31:15.762848","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val_loss = float('inf')\npatience = 8\npatience_counter = 0\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    train_loss, train_mask_loss = train_one_epoch(\n        model, optimizer, dl_val, DEVICE, epoch, \n        warmup_scheduler if epoch <= WARMUP_EPOCHS else None\n    )\n    \n    val_loss, val_mask_loss = validate_one_epoch(model, dl_val, DEVICE)\n    \n    if epoch > WARMUP_EPOCHS:\n        main_scheduler.step()\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    \n    print(f\"Epoch {epoch:2d}/{NUM_EPOCHS} - Train Loss: {train_loss:.4f}\")\n    print(f\"                 Train Mask: {train_mask_loss:.4f}\")\n    print(f\"                 LR: {optimizer.param_groups[0]['lr']:.6f}\")\n    \n    # In training loop, replace the save lines with:\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        model_to_save = model.module if hasattr(model, 'module') else model\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model_to_save.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n        }, 'best_model.pth')\n        print(f\"                 New best model saved!\")\n    \n    # And for regular checkpoints:\n    model_to_save = model.module if hasattr(model, 'module') else model\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model_to_save.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'val_loss': val_loss,\n    }, f'model_epoch_{epoch}.pth')\n    \n    print(\"-\" * 50)","metadata":{"papermill":{"duration":9768.377064,"end_time":"2025-07-12T23:14:04.385932","exception":false,"start_time":"2025-07-12T20:31:16.008868","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CellTestDataset(Dataset):\n    def __init__(self, image_dir, transforms=None):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.image_ids = [fname[:-4] for fname in os.listdir(self.image_dir) \n                          if fname.endswith('.png')]\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_path = os.path.join(self.image_dir, image_id + '.png')\n        img = Image.open(image_path).convert(\"RGB\")\n        img_np = np.array(img)\n\n        if self.transforms:\n            augmented = self.transforms(image=img_np, bboxes=[], labels=[])\n            img_tensor = augmented['image']\n        else:\n            img_tensor = torchvision.transforms.ToTensor()(img)\n\n        return {'image': img_tensor, 'image_id': image_id}\n\n    def __len__(self):\n        return len(self.image_ids)\n\nds_test = CellTestDataset(TEST_PATH, transforms=get_transform(train=False))\ntest_loader = DataLoader(ds_test, batch_size=1, shuffle=False, num_workers=2, collate_fn=lambda x: x)","metadata":{"papermill":{"duration":0.297701,"end_time":"2025-07-12T23:14:05.023128","exception":false,"start_time":"2025-07-12T23:14:04.725427","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/input/model_epoch_20/pytorch/default/1/model_epoch_20.pth')\nmodel = get_model()\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel = model.to(DEVICE)\n\nmodel.eval()\n\nprint(f\"Loaded best model from epoch {checkpoint['epoch']}\")","metadata":{"papermill":{"duration":1.812398,"end_time":"2025-07-12T23:14:07.099626","exception":false,"start_time":"2025-07-12T23:14:05.287228","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_nms_to_masks(boxes, scores, masks, iou_threshold=0.3):\n    keep = torchvision.ops.nms(boxes, scores, iou_threshold)\n    return boxes[keep], scores[keep], masks[keep]\n\ndef post_process_masks(masks, min_area=50):\n    processed_masks = []\n    for mask in masks:\n        mask_np = mask.cpu().numpy()\n        if mask_np.sum() < min_area:\n            continue\n        processed_masks.append(mask_np)\n    return processed_masks\n\nsubmission = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Inference\"):\n        sample = batch[0]\n        img = sample['image'].to(DEVICE)\n        image_id = sample['image_id']\n        \n        outputs = model([img])[0]\n        \n        if len(outputs['masks']) == 0:\n            submission.append((image_id, \"-1\"))\n            continue\n        \n        boxes = outputs['boxes']\n        scores = outputs['scores']\n        masks = outputs['masks']\n        \n        mask_above_threshold = scores > BOXES_CONF\n        if not mask_above_threshold.any():\n            submission.append((image_id, \"-1\"))\n            continue\n            \n        boxes = boxes[mask_above_threshold]\n        scores = scores[mask_above_threshold]\n        masks = masks[mask_above_threshold]\n        \n        boxes, scores, masks = apply_nms_to_masks(boxes, scores, masks)\n        \n        any_mask = False\n        prev_masks = []\n        \n        for mask, score in zip(masks, scores):\n            mask_np = mask.cpu().numpy()\n            bin_mask = mask_np[0] > MASK_THRESHOLD\n            \n            if bin_mask.sum() < 20:\n                continue\n                \n            bin_mask = remove_overlapping_pixels(bin_mask, prev_masks)\n            \n            if bin_mask.sum() < 10:\n                continue\n                \n            prev_masks.append(bin_mask)\n            rle = rle_encoding(bin_mask.astype(np.uint8))\n            \n            if rle:\n                submission.append((image_id, rle))\n                any_mask = True\n        \n        if not any_mask:\n            submission.append((image_id, \"-1\"))\n\ndf_sub = pd.DataFrame(submission, columns=['id', 'annotation'])\ndf_sub[\"idx\"] = range(len(df_sub))\ndf_sub = df_sub[[\"idx\", \"id\", \"annotation\"]].replace({\"\": \"-1\"})\ndf_sub.to_csv(\"submission.csv\", index=False)\n\nprint(f\"Submission created with {len(df_sub)} entries\")\nprint(f\"Images with predictions: {len(df_sub[df_sub['annotation'] != '-1'])}\")\nprint(f\"Images without predictions: {len(df_sub[df_sub['annotation'] == '-1'])}\")","metadata":{"papermill":{"duration":194.376788,"end_time":"2025-07-12T23:17:21.748500","exception":false,"start_time":"2025-07-12T23:14:07.371712","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader\nimport torch\nimport math\n\ndef show_test_predictions(model, loader, device, num_samples=3,\n                          mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225),\n                          mask_alpha=0.4, box_conf=BOXES_CONF, mask_thr=MASK_THRESHOLD):\n    model.eval()\n    fig, axes = plt.subplots(num_samples, 2, \n                             figsize=(10, 5 * num_samples))\n    axes = axes.reshape(num_samples, 2)\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(loader):\n            if idx >= num_samples:\n                break\n            sample = batch[0]\n            img_t = sample['image'].to(device)     \n            image_id = sample['image_id']\n            \n            # 2) Forward pass\n            outputs = model([img_t])[0]\n            \n            # 3) Un-normalize for display\n            img_disp = img_t.cpu().clone()\n            for c in range(3):\n                img_disp[c] = img_disp[c] * std[c] + mean[c]\n            img_np = img_disp.permute(1,2,0).numpy()\n            img_np = np.clip(img_np, 0, 1)\n            \n            # 4) Build combined predicted mask\n            combined_mask = np.zeros(img_np.shape[:2], dtype=bool)\n            prev = []\n            for mask, score in zip(outputs['masks'], outputs['scores']):\n                if score.item() < box_conf:\n                    continue\n                m = mask.cpu().numpy()[0] > mask_thr\n                # remove overlaps if needed\n                m = remove_overlapping_pixels(m, prev)\n                prev.append(m)\n                combined_mask = np.logical_or(combined_mask, m)\n            \n            # 5) Plot original\n            ax_img = axes[idx, 0]\n            ax_img.imshow(img_np)\n            ax_img.set_title(f\"Image: {image_id}\")\n            ax_img.axis('off')\n            \n            # 6) Plot prediction\n            ax_pred = axes[idx, 1]\n            ax_pred.imshow(img_np)\n            ax_pred.imshow(combined_mask, alpha=mask_alpha, cmap='cividis')\n            ax_pred.set_title(\"Predicted Mask\")\n            ax_pred.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nshow_test_predictions(model, test_loader, DEVICE, num_samples=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}